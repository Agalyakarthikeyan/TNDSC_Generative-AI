{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 18178,
          "sourceType": "datasetVersion",
          "datasetId": 13433
        },
        {
          "sourceId": 13808,
          "sourceType": "datasetVersion",
          "datasetId": 9714
        }
      ],
      "dockerImageVersionId": 30673,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "PxkiAFctsZwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2CMX2rHVsZwP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Attention Class**"
      ],
      "metadata": {
        "id": "mNnMcy3rsZwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "from tensorflow.python.keras.layers import Layer\n",
        "from tensorflow.python.keras import backend as K\n",
        "\n",
        "\n",
        "class AttentionLayer(Layer):\n",
        "    \"\"\"\n",
        "    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n",
        "    There are three sets of weights introduced W_a, U_a, and V_a\n",
        "     \"\"\"\n",
        "def __init__(self, **kwargs):\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert isinstance(input_shape, list)\n",
        "        # Create a trainable weight variable for this layer.\n",
        "\n",
        "        self.W_a = self.add_weight(name='W_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.U_a = self.add_weight(name='U_a',\n",
        "                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.V_a = self.add_weight(name='V_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n",
        "\n",
        "    def call(self, inputs, verbose=False):\n",
        "        \"\"\"\n",
        "        inputs: [encoder_output_sequence, decoder_output_sequence]\n",
        "        \"\"\"\n",
        "        assert type(inputs) == list\n",
        "        encoder_out_seq, decoder_out_seq = inputs\n",
        "        if verbose:\n",
        "            print('encoder_out_seq>', encoder_out_seq.shape)\n",
        "            print('decoder_out_seq>', decoder_out_seq.shape)\n",
        "\n",
        "        def energy_step(inputs, states):\n",
        "            \"\"\" Step function for computing energy for a single decoder state\n",
        "            inputs: (batchsize * 1 * de_in_dim)\n",
        "            states: (batchsize * 1 * de_latent_dim)\n",
        "            \"\"\"\n",
        "\n",
        "            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n",
        "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
        "\n",
        "            \"\"\" Some parameters required for shaping tensors\"\"\"\n",
        "            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n",
        "            de_hidden = inputs.shape[-1]\n",
        "\n",
        "            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n",
        "            # <= batch size * en_seq_len * latent_dim\n",
        "            W_a_dot_s = K.dot(encoder_out_seq, self.W_a)\n",
        "\n",
        "            \"\"\" Computing hj.Ua \"\"\"\n",
        "            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n",
        "            if verbose:\n",
        "                print('Ua.h>', U_a_dot_h.shape)\n",
        "                 print('Ua.h>', U_a_dot_h.shape)\n",
        "\n",
        "            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            Ws_plus_Uh = K.tanh(W_a_dot_s + U_a_dot_h)\n",
        "            if verbose:\n",
        "                print('Ws+Uh>', Ws_plus_Uh.shape)\n",
        "\n",
        "            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.squeeze(K.dot(Ws_plus_Uh, self.V_a), axis=-1)\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.softmax(e_i)\n",
        "\n",
        "            if verbose:\n",
        "                print('ei>', e_i.shape)\n",
        "\n",
        "            return e_i, [e_i]\n",
        "        def context_step(inputs, states):\n",
        "            \"\"\" Step function for computing ci using ei \"\"\"\n",
        "\n",
        "            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n",
        "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
        "\n",
        "            # <= batch_size, hidden_size\n",
        "            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n",
        "            if verbose:\n",
        "                print('ci>', c_i.shape)\n",
        "            return c_i, [c_i]\n",
        "\n",
        "        fake_state_c = K.sum(encoder_out_seq, axis=1)\n",
        "        fake_state_e = K.sum(encoder_out_seq, axis=2)  # <= (batch_size, enc_seq_len, latent_dim\n",
        "\n",
        "        \"\"\" Computing energy outputs \"\"\"\n",
        "        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n",
        "        last_out, e_outputs, _ = K.rnn(\n",
        "            energy_step, decoder_out_seq, [fake_state_e],\n",
        "        )\n",
        "\n",
        "        \"\"\" Computing context vectors \"\"\"\n",
        "        last_out, c_outputs, _ = K.rnn(\n",
        "            context_step, e_outputs, [fake_state_c],\n",
        "        )\n",
        "\n",
        "        return c_outputs, e_outputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        \"\"\" Outputs produced by the layer \"\"\"\n",
        "        return [\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_sape[1][2])),\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n",
        "        ]"
      ],
      "metadata": {
        "id": "Hj7ZEwOzsZwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "lines = open('../input/chatbot-data/cornell movie-dialogs corpus/movie_lines.txt', encoding='utf-8',\n",
        "             errors='ignore').read().split('\\n')\n",
        "\n",
        "convers = open('../input/chatbot-data/cornell movie-dialogs corpus/movie_conversations.txt', encoding='utf-8',\n",
        "             errors='ignore').read().split('\\n')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-31T17:34:37.960549Z",
          "iopub.execute_input": "2024-03-31T17:34:37.960967Z",
          "iopub.status.idle": "2024-03-31T17:34:38.451125Z",
          "shell.execute_reply.started": "2024-03-31T17:34:37.960932Z",
          "shell.execute_reply": "2024-03-31T17:34:38.449752Z"
        },
        "trusted": true,
        "id": "2oTMuGkUsZwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(lines)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-31T17:34:38.453303Z",
          "iopub.execute_input": "2024-03-31T17:34:38.453743Z",
          "iopub.status.idle": "2024-03-31T17:34:38.461373Z",
          "shell.execute_reply.started": "2024-03-31T17:34:38.453708Z",
          "shell.execute_reply": "2024-03-31T17:34:38.460519Z"
        },
        "trusted": true,
        "id": "VcmIVlC4sZwT",
        "outputId": "c14b6420-2cca-4684-b5d8-a43ff0f3d0cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 2,
          "output_type": "execute_result",
          "data": {
            "text/plain": "304714"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "exchn = []\n",
        "for conver in convers:\n",
        "    exchn.append(conver.split(' +++$+++ ')[-1][1:-1].replace(\"'\", \" \").replace(\",\",\"\").split())\n",
        "\n",
        "diag = {}\n",
        "for line in lines:\n",
        "    diag[line.split(' +++$+++ ')[0]] = line.split(' +++$+++ ')[-1]\n",
        "\n",
        "\n",
        "\n",
        "## delete\n",
        "del(lines, convers, conver, line)\n",
        "\n",
        "\n",
        "\n",
        "questions = []\n",
        "answers = []\n",
        "\n",
        "for conver in exchn:\n",
        "    for i in range(len(conver) - 1):\n",
        "        questions.append(diag[conver[i]])\n",
        "        answers.append(diag[conver[i+1]])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## delete\n",
        "del(diag, exchn, conver, i)\n",
        "\n",
        "\n",
        "###############################\n",
        "#        max_len = 13         #\n",
        "###############################\n",
        "\n",
        "sorted_ques = []\n",
        "sorted_ans = []\n",
        "for i in range(len(questions)):\n",
        "    if len(questions[i]) < 13:\n",
        "        sorted_ques.append(questions[i])\n",
        "        sorted_ans.append(answers[i])\n",
        "\n",
        "\n",
        "\n",
        "###############################\n",
        "#                             #\n",
        "###############################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def clean_text(txt):\n",
        "    txt = txt.lower()\n",
        "    txt = re.sub(r\"i'm\", \"i am\", txt)\n",
        "    txt = re.sub(r\"he's\", \"he is\", txt)\n",
        "    txt = re.sub(r\"she's\", \"she is\", txt)\n",
        "    txt = re.sub(r\"that's\", \"that is\", txt)\n",
        "    txt = re.sub(r\"what's\", \"what is\", txt)\n",
        "    txt = re.sub(r\"where's\", \"where is\", txt)\n",
        "    txt = re.sub(r\"\\'ll\", \" will\", txt)\n",
        "    txt = re.sub(r\"\\'ve\", \" have\", txt)\n",
        "    txt = re.sub(r\"\\'re\", \" are\", txt)\n",
        "    txt = re.sub(r\"\\'d\", \" would\", txt)\n",
        "    txt = re.sub(r\"won't\", \"will not\", txt)\n",
        "    txt = re.sub(r\"can't\", \"can not\", txt)\n",
        "    txt = re.sub(r\"[^\\w\\s]\", \"\", txt)\n",
        "    return txt\n",
        "\n",
        "clean_ques = []\n",
        "clean_ans = []\n",
        "\n",
        "for line in sorted_ques:\n",
        "    clean_ques.append(clean_text(line))\n",
        "\n",
        "for line in sorted_ans:\n",
        "    clean_ans.append(clean_text(line))\n",
        "\n",
        "\n",
        "\n",
        "## delete\n",
        "del(answers, questions, line)\n",
        "\n",
        "\n",
        "###############################\n",
        "#                             #\n",
        "###############################\n",
        "\n",
        "\n",
        "for i in range(len(clean_ans)):\n",
        "    clean_ans[i] = ' '.join(clean_ans[i].split()[:11])\n",
        "\n",
        "\n",
        "\n",
        "###############################\n",
        "#                             #\n",
        "###############################\n",
        "\n",
        "del(sorted_ans, sorted_ques)\n",
        "\n",
        "\n",
        "## trimming\n",
        "clean_ans=clean_ans[:30000]\n",
        "clean_ques=clean_ques[:30000]\n",
        "## delete\n",
        "\n",
        "\n",
        "###  count occurences ###\n",
        "word2count = {}\n",
        "\n",
        "for line in clean_ques:\n",
        "    for word in line.split():\n",
        "        if word not in word2count:\n",
        "            word2count[word] = 1\n",
        "        else:\n",
        "            word2count[word] += 1\n",
        "for line in clean_ans:\n",
        "    for word in line.split():\n",
        "        if word not in word2count:\n",
        "            word2count[word] = 1\n",
        "        else:\n",
        "            word2count[word] += 1\n",
        "\n",
        "## delete\n",
        "del(word, line)\n",
        "\n",
        "\n",
        "###  remove less frequent ###\n",
        "thresh = 5\n",
        "\n",
        "vocab = {}\n",
        "word_num = 0\n",
        "for word, count in word2count.items():\n",
        "    if count >= thresh:\n",
        "        vocab[word] = word_num\n",
        "        word_num += 1\n",
        "\n",
        "## delete\n",
        "del(word2count, word, count, thresh)\n",
        "del(word_num)\n",
        "\n",
        "\n",
        "\n",
        "for i in range(len(clean_ans)):\n",
        "    clean_ans[i] = '<SOS> ' + clean_ans[i] + ' <EOS>'\n",
        "\n",
        "\n",
        "\n",
        "tokens = ['<PAD>', '<EOS>', '<OUT>', '<SOS>']\n",
        "x = len(vocab)\n",
        "for token in tokens:\n",
        "    vocab[token] = x\n",
        "    x += 1\n",
        "\n",
        "\n",
        "\n",
        "vocab['cameron'] = vocab['<PAD>']\n",
        "vocab['<PAD>'] = 0\n",
        "\n",
        "## delete\n",
        "del(token, tokens)\n",
        "del(x)\n",
        "\n",
        "### inv answers dict ###\n",
        "inv_vocab = {w:v for v, w in vocab.items()}\n",
        "\n",
        "\n",
        "\n",
        "## delete\n",
        "del(i)\n",
        "\n",
        "\n",
        "\n",
        "encoder_inp = []\n",
        "for line in clean_ques:\n",
        "    lst = []\n",
        "    for word in line.split():\n",
        "        if word not in vocab:\n",
        "            lst.append(vocab['<OUT>'])\n",
        "        else:\n",
        "            lst.append(vocab[word])\n",
        "\n",
        "    encoder_inp.append(lst)\n",
        "\n",
        "decoder_inp = []\n",
        "for line in clean_ans:\n",
        "    lst = []\n",
        "    for word in line.split():\n",
        "        if word not in vocab:\n",
        "            lst.append(vocab['<OUT>'])\n",
        "        else:\n",
        "            lst.append(vocab[word])\n",
        "    decoder_inp.append(lst)\n",
        "\n",
        "### delete\n",
        "del(clean_ans, clean_ques, line, lst, word)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "encoder_inp = pad_sequences(encoder_inp, 13, padding='post', truncating='post')\n",
        "decoder_inp = pad_sequences(decoder_inp, 13, padding='post', truncating='post')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "decoder_final_output = []\n",
        "for i in decoder_inp:\n",
        "    decoder_final_output.append(i[1:])\n",
        "\n",
        "decoder_final_output = pad_sequences(decoder_final_output, 13, padding='post', truncating='post')\n",
        "\n",
        "\n",
        "del(i)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-31T17:38:51.670595Z",
          "iopub.execute_input": "2024-03-31T17:38:51.67099Z",
          "iopub.status.idle": "2024-03-31T17:39:09.431448Z",
          "shell.execute_reply.started": "2024-03-31T17:38:51.670962Z",
          "shell.execute_reply": "2024-03-31T17:39:09.43045Z"
        },
        "trusted": true,
        "id": "TWTXg40GsZwU",
        "outputId": "c265de81-5723-4c06-d62e-587b266ea100"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "2024-03-31 17:38:56.758308: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-31 17:38:56.758474: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-31 17:38:56.916588: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# decoder_final_output, decoder_final_input, encoder_final, vocab, inv_vocab\n",
        "\n",
        "VOCAB_SIZE = len(vocab)\n",
        "MAX_LEN = 13\n",
        "\n",
        "print(decoder_final_output.shape, decoder_inp.shape, encoder_inp.shape, len(vocab), len(inv_vocab), inv_vocab[0])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-31T17:39:19.345784Z",
          "iopub.execute_input": "2024-03-31T17:39:19.346391Z",
          "iopub.status.idle": "2024-03-31T17:39:19.35226Z",
          "shell.execute_reply.started": "2024-03-31T17:39:19.346362Z",
          "shell.execute_reply": "2024-03-31T17:39:19.35114Z"
        },
        "trusted": true,
        "id": "HDndGKY5sZwW",
        "outputId": "24174ebf-7e63-43ce-dfd7-0a890f4a565e"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "(30000, 13) (30000, 13) (30000, 13) 3027 3027 <PAD>\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inv_vocab[16]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-31T17:39:39.050053Z",
          "iopub.execute_input": "2024-03-31T17:39:39.050743Z",
          "iopub.status.idle": "2024-03-31T17:39:39.058118Z",
          "shell.execute_reply.started": "2024-03-31T17:39:39.050711Z",
          "shell.execute_reply": "2024-03-31T17:39:39.056941Z"
        },
        "trusted": true,
        "id": "9nbp-MSWsZwW",
        "outputId": "8411b8ad-e690-4151-f8a3-149c8e97dc27"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 6,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'they'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print(len(decoder_final_input), MAX_LEN, VOCAB_SIZE)\n",
        "#decoder_final_input[0]\n",
        "#decoder_output_data = np.zeros((len(decoder_final_input), MAX_LEN, VOCAB_SIZE), dtype=\"float32\")\n",
        "#print(decoder_output_data.shape)\n",
        "#decoder_final_input[80]"
      ],
      "metadata": {
        "id": "CnEygYzHsZwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "decoder_final_output = to_categorical(decoder_final_output, len(vocab))"
      ],
      "metadata": {
        "id": "OaJ59n8FsZwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_final_output.shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-31T17:40:28.079674Z",
          "iopub.execute_input": "2024-03-31T17:40:28.080062Z",
          "iopub.status.idle": "2024-03-31T17:40:28.086688Z",
          "shell.execute_reply.started": "2024-03-31T17:40:28.080034Z",
          "shell.execute_reply": "2024-03-31T17:40:28.085575Z"
        },
        "trusted": true,
        "id": "lfAa5M89sZwY",
        "outputId": "d867db27-d0f3-4889-ca1c-792c63ad881e"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 7,
          "output_type": "execute_result",
          "data": {
            "text/plain": "(30000, 13)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GLOVE EMBEDDING**"
      ],
      "metadata": {
        "id": "N92BW0rjsZwY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "embeddings_index = {}\n",
        "\n",
        "with open('../input/glove6b50d/glove.6B.50d.txt', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Glove Loaded!\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-31T17:42:59.770189Z",
          "iopub.execute_input": "2024-03-31T17:42:59.771248Z",
          "iopub.status.idle": "2024-03-31T17:43:08.328321Z",
          "shell.execute_reply.started": "2024-03-31T17:42:59.771197Z",
          "shell.execute_reply": "2024-03-31T17:43:08.327327Z"
        },
        "trusted": true,
        "id": "HWOvuSE0sZwZ",
        "outputId": "9ccb952f-e414-473a-e166-d134c623220a"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Glove Loaded!\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dimention = 50\n",
        "def embedding_matrix_creater(embedding_dimention, word_index):\n",
        "    embedding_matrix = np.zeros((len(word_index)+1, embedding_dimention))\n",
        "    for word, i in word_index.items():\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "          # words not found in embedding index will be all-zeros.\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "    return embedding_matrix\n",
        "embedding_matrix = embedding_matrix_creater(50, word_index=vocab)\n",
        "del(embeddings_index)\n",
        "embedding_matrix.shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-31T17:47:00.954638Z",
          "iopub.execute_input": "2024-03-31T17:47:00.955476Z",
          "iopub.status.idle": "2024-03-31T17:47:00.966776Z",
          "shell.execute_reply.started": "2024-03-31T17:47:00.955443Z",
          "shell.execute_reply": "2024-03-31T17:47:00.965774Z"
        },
        "trusted": true,
        "id": "pqJ6g6iQsZwZ",
        "outputId": "a93fdf03-add2-43fb-ebb3-1c5c8b87a72a"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 22,
          "output_type": "execute_result",
          "data": {
            "text/plain": "(3028, 50)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix[0]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-31T17:47:31.550334Z",
          "iopub.execute_input": "2024-03-31T17:47:31.550729Z",
          "iopub.status.idle": "2024-03-31T17:47:31.557447Z",
          "shell.execute_reply.started": "2024-03-31T17:47:31.550701Z",
          "shell.execute_reply": "2024-03-31T17:47:31.556619Z"
        },
        "trusted": true,
        "id": "sXlfb6rfsZwZ",
        "outputId": "f2529333-53b0-4a82-9746-f69ee8094e70"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 23,
          "output_type": "execute_result",
          "data": {
            "text/plain": "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, Input, Bidirectional, Concatenate, Dropout, Attention"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-31T17:48:06.703873Z",
          "iopub.execute_input": "2024-03-31T17:48:06.704337Z",
          "iopub.status.idle": "2024-03-31T17:48:06.712758Z",
          "shell.execute_reply.started": "2024-03-31T17:48:06.704304Z",
          "shell.execute_reply": "2024-03-31T17:48:06.711774Z"
        },
        "trusted": true,
        "id": "NNsE9PwjsZwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-31T17:55:47.40713Z",
          "iopub.execute_input": "2024-03-31T17:55:47.407993Z",
          "iopub.status.idle": "2024-03-31T17:55:47.412843Z",
          "shell.execute_reply.started": "2024-03-31T17:55:47.407959Z",
          "shell.execute_reply": "2024-03-31T17:55:47.411753Z"
        },
        "trusted": true,
        "id": "WG8k24EHsZwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FxqIyeKMsZwb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "id": "0r3bat02sl40",
        "outputId": "30c8257e-e51c-4136-a869-67b7bbe5924e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.10.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.62.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "RFOwnhhIsosu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc_inp = tf.keras.layers.Input(shape=(13, ))"
      ],
      "metadata": {
        "id": "mpWN5jD8sr1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc_inp = Input(shape=(13, ))\n",
        "enc_model = tf.keras.models.Model(enc_inp, [encoder_outputs, enc_states])\n",
        "\n",
        "\n",
        "decoder_state_input_h = tf.keras.layers.Input(shape=( 400 * 2,))\n",
        "decoder_state_input_c = tf.keras.layers.Input(shape=( 400 * 2,))\n",
        "\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "\n",
        "decoder_outputs, state_h, state_c = dec_lstm(dec_embed , initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "\n",
        "#decoder_output = dec_dense(decoder_outputs)\n",
        "\n",
        "dec_model = tf.keras.models.Model([dec_inp, decoder_states_inputs],\n",
        "                                      [decoder_outputs] + decoder_states)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-31T18:06:38.550376Z",
          "iopub.execute_input": "2024-03-31T18:06:38.551177Z",
          "iopub.status.idle": "2024-03-31T18:06:38.602007Z",
          "shell.execute_reply.started": "2024-03-31T18:06:38.551144Z",
          "shell.execute_reply": "2024-03-31T18:06:38.600325Z"
        },
        "trusted": true,
        "id": "0BnXu1OLsZwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, TimeDistributed"
      ],
      "metadata": {
        "id": "Uc-DQ32us4zM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input"
      ],
      "metadata": {
        "id": "KiVpyAVZs7GD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "print(\"##########################################\")\n",
        "print(\"#       start chatting ver. 1.0          #\")\n",
        "print(\"##########################################\")\n",
        "\n",
        "\n",
        "prepro1 = \"\"\n",
        "while prepro1 != 'q':\n",
        "\n",
        "    prepro1 = input(\"you : \")\n",
        "    try:\n",
        "        prepro1 = clean_text(prepro1)\n",
        "        prepro = [prepro1]\n",
        "\n",
        "        txt = []\n",
        "        for x in prepro:\n",
        "            lst = []\n",
        "            for y in x.split():\n",
        "                try:\n",
        "                    lst.append(vocab[y])\n",
        "                except:\n",
        "                    lst.append(vocab['<OUT>'])\n",
        "            txt.append(lst)\n",
        "        txt = pad_sequences(txt, 13, padding='post')\n",
        "\n",
        "\n",
        "        ###\n",
        "        enc_op, stat = enc_model.predict( txt )\n",
        "\n",
        "        empty_target_seq = np.zeros( ( 1 , 1) )\n",
        "        empty_target_seq[0, 0] = vocab['<SOS>']\n",
        "        stop_condition = False\n",
        "        decoded_translation = ''\n",
        "\n",
        "\n",
        "        while not stop_condition :\n",
        "\n",
        "            dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + stat )\n",
        "\n",
        "            ###\n",
        "            ###########################\n",
        "            attn_op, attn_state = attn_layer([enc_op, dec_outputs])\n",
        "            decoder_concat_input = Concatenate(axis=-1)([dec_outputs, attn_op])\n",
        "            decoder_concat_input = dec_dense(decoder_concat_input)\n",
        "            ###########################\n",
        "\n",
        "            sampled_word_index = np.argmax( decoder_concat_input[0, -1, :] )\n",
        "\n",
        "            sampled_word = inv_vocab[sampled_word_index] + ' '\n",
        "\n",
        "            if sampled_word != '<EOS> ':\n",
        "                decoded_translation += sampled_word\n",
        "\n",
        "\n",
        "            if sampled_word == '<EOS> ' or len(decoded_translation.split()) > 13:\n",
        "                stop_condition = True\n",
        "\n",
        "            empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
        "            empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
        "            stat = [ h , c ]\n",
        "\n",
        "        print(\"chatbot attention : \", decoded_translation )\n",
        "        print(\"==============================================\")\n",
        "\n",
        "    except:\n",
        "        print(\"sorry didn't got you , please type again :( \")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-31T18:03:09.545312Z",
          "iopub.execute_input": "2024-03-31T18:03:09.546524Z",
          "iopub.status.idle": "2024-03-31T18:06:03.161698Z",
          "shell.execute_reply.started": "2024-03-31T18:03:09.546488Z",
          "shell.execute_reply": "2024-03-31T18:06:03.160002Z"
        },
        "trusted": true,
        "id": "LZJorFItsZwc",
        "outputId": "20c9d827-a25b-4f4d-c061-c9dac3fc1af5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##########################################\n",
            "#       start chatting ver. 1.0          #\n",
            "##########################################\n",
            "sorry didn't got you , please type again :( \n",
            "you : hi\n",
            "sorry didn't got you , please type again :( \n"
          ]
        }
      ]
    }
  ]
}